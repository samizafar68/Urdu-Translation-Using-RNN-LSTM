{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCES</th>\n",
       "      <th>MEANING</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "      <th>Unnamed: 25</th>\n",
       "      <th>Unnamed: 26</th>\n",
       "      <th>Unnamed: 27</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "      <th>Unnamed: 29</th>\n",
       "      <th>Unnamed: 30</th>\n",
       "      <th>Unnamed: 31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I communicate with my parents?</td>\n",
       "      <td>میں اپنے والدین سے کیسے بات کروں ؟</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I make friends?’</td>\n",
       "      <td>میں دوست کیسے بنائوں ؟</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do I get so sad?’</td>\n",
       "      <td>میں اتنا اداس کیوں ہوں؟.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you’ve asked yourself such questions, you’r...</td>\n",
       "      <td>اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Depending on where you’ve turned for guidance,...</td>\n",
       "      <td>اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>To help young people get solid advice they can...</td>\n",
       "      <td>نوجوانوں کو ٹھوس مشورے حاصل کرنے میں مدد کرنے ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in January1982. Decades later, the series stil...</td>\n",
       "      <td>8 جنوری 1982۔ دہائیوں کے بعد، سیریز اب بھی ایک...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Each article is the product of extensive resea...</td>\n",
       "      <td>درحقیقت، اس بات کا تعین کرنے کے لیے کہ نوجوان ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The book you now hold was originally published...</td>\n",
       "      <td>جو کتاب آپ کے پاس ہے وہ اصل میں 1989 میں شائع ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>However, the chapters have been completely rev...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          SENTENCES   \\\n",
       "0             How can I communicate with my parents?   \n",
       "1                           How can I make friends?’   \n",
       "2                              Why do I get so sad?’   \n",
       "3  If you’ve asked yourself such questions, you’r...   \n",
       "4  Depending on where you’ve turned for guidance,...   \n",
       "5  To help young people get solid advice they can...   \n",
       "6  in January1982. Decades later, the series stil...   \n",
       "7  Each article is the product of extensive resea...   \n",
       "8  The book you now hold was originally published...   \n",
       "9  However, the chapters have been completely rev...   \n",
       "\n",
       "                                             MEANING  Unnamed: 2  Unnamed: 3  \\\n",
       "0                 میں اپنے والدین سے کیسے بات کروں ؟         NaN         NaN   \n",
       "1                             میں دوست کیسے بنائوں ؟         NaN         NaN   \n",
       "2                           میں اتنا اداس کیوں ہوں؟.         NaN         NaN   \n",
       "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...         NaN         NaN   \n",
       "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...         NaN         NaN   \n",
       "5  نوجوانوں کو ٹھوس مشورے حاصل کرنے میں مدد کرنے ...         NaN         NaN   \n",
       "6  8 جنوری 1982۔ دہائیوں کے بعد، سیریز اب بھی ایک...         NaN         NaN   \n",
       "7  درحقیقت، اس بات کا تعین کرنے کے لیے کہ نوجوان ...         NaN         NaN   \n",
       "8  جو کتاب آپ کے پاس ہے وہ اصل میں 1989 میں شائع ...         NaN         NaN   \n",
       "9                                                ...         NaN         NaN   \n",
       "\n",
       "   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
       "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "5         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "6         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "7         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "8         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "9         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   ...  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  \\\n",
       "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "5  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "6  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "7  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "8  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "9  ...          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 27  Unnamed: 28 Unnamed: 29  Unnamed: 30 Unnamed: 31  \n",
       "0          NaN          NaN         NaN          NaN         NaN  \n",
       "1          NaN          NaN         NaN          NaN         NaN  \n",
       "2          NaN          NaN         NaN          NaN         NaN  \n",
       "3          NaN          NaN         NaN          NaN         NaN  \n",
       "4          NaN          NaN         NaN          NaN         NaN  \n",
       "5          NaN          NaN         NaN          NaN         NaN  \n",
       "6          NaN          NaN         NaN          NaN         NaN  \n",
       "7          NaN          NaN         NaN          NaN         NaN  \n",
       "8          NaN          NaN         NaN          NaN         NaN  \n",
       "9          NaN          NaN         NaN          NaN         NaN  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data\n",
    "file_path = 'parallel-corpus.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first 10 lines of Datset\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCES</th>\n",
       "      <th>MEANING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I communicate with my parents?</td>\n",
       "      <td>میں اپنے والدین سے کیسے بات کروں ؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I make friends?’</td>\n",
       "      <td>میں دوست کیسے بنائوں ؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do I get so sad?’</td>\n",
       "      <td>میں اتنا اداس کیوں ہوں؟.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you’ve asked yourself such questions, you’r...</td>\n",
       "      <td>اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Depending on where you’ve turned for guidance,...</td>\n",
       "      <td>اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>To help young people get solid advice they can...</td>\n",
       "      <td>نوجوانوں کو ٹھوس مشورے حاصل کرنے میں مدد کرنے ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in January1982. Decades later, the series stil...</td>\n",
       "      <td>8 جنوری 1982۔ دہائیوں کے بعد، سیریز اب بھی ایک...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Each article is the product of extensive resea...</td>\n",
       "      <td>درحقیقت، اس بات کا تعین کرنے کے لیے کہ نوجوان ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The book you now hold was originally published...</td>\n",
       "      <td>جو کتاب آپ کے پاس ہے وہ اصل میں 1989 میں شائع ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>However, the chapters have been completely rev...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          SENTENCES   \\\n",
       "0             How can I communicate with my parents?   \n",
       "1                           How can I make friends?’   \n",
       "2                              Why do I get so sad?’   \n",
       "3  If you’ve asked yourself such questions, you’r...   \n",
       "4  Depending on where you’ve turned for guidance,...   \n",
       "5  To help young people get solid advice they can...   \n",
       "6  in January1982. Decades later, the series stil...   \n",
       "7  Each article is the product of extensive resea...   \n",
       "8  The book you now hold was originally published...   \n",
       "9  However, the chapters have been completely rev...   \n",
       "\n",
       "                                             MEANING  \n",
       "0                 میں اپنے والدین سے کیسے بات کروں ؟  \n",
       "1                             میں دوست کیسے بنائوں ؟  \n",
       "2                           میں اتنا اداس کیوں ہوں؟.  \n",
       "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...  \n",
       "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...  \n",
       "5  نوجوانوں کو ٹھوس مشورے حاصل کرنے میں مدد کرنے ...  \n",
       "6  8 جنوری 1982۔ دہائیوں کے بعد، سیریز اب بھی ایک...  \n",
       "7  درحقیقت، اس بات کا تعین کرنے کے لیے کہ نوجوان ...  \n",
       "8  جو کتاب آپ کے پاس ہے وہ اصل میں 1989 میں شائع ...  \n",
       "9                                                ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove extra columns, keeping only 'SENTENCES ' and 'MEANING'\n",
    "df_cleaned = df[['SENTENCES ', 'MEANING']]\n",
    "\n",
    "# Display the specific index data (e.g., index 6403) from 'SENTENCES ' column\n",
    "df_cleaned.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emoji_Removed_english</th>\n",
       "      <th>Emoji_Removed_urdu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I communicate with my parents?</td>\n",
       "      <td>میں اپنے والدین سے کیسے بات کروں ؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I make friends?’</td>\n",
       "      <td>میں دوست کیسے بنائوں ؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do I get so sad?’</td>\n",
       "      <td>میں اتنا اداس کیوں ہوں؟.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you’ve asked yourself such questions, you’r...</td>\n",
       "      <td>اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Depending on where you’ve turned for guidance,...</td>\n",
       "      <td>اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>To help young people get solid advice they can...</td>\n",
       "      <td>نوجوانوں کو ٹھوس مشورے حاصل کرنے میں مدد کرنے ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in January1982. Decades later, the series stil...</td>\n",
       "      <td>8 جنوری 1982۔ دہائیوں کے بعد، سیریز اب بھی ایک...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Each article is the product of extensive resea...</td>\n",
       "      <td>درحقیقت، اس بات کا تعین کرنے کے لیے کہ نوجوان ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The book you now hold was originally published...</td>\n",
       "      <td>جو کتاب آپ کے پاس ہے وہ اصل میں 1989 میں شائع ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>However, the chapters have been completely rev...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Emoji_Removed_english  \\\n",
       "0             How can I communicate with my parents?   \n",
       "1                           How can I make friends?’   \n",
       "2                              Why do I get so sad?’   \n",
       "3  If you’ve asked yourself such questions, you’r...   \n",
       "4  Depending on where you’ve turned for guidance,...   \n",
       "5  To help young people get solid advice they can...   \n",
       "6  in January1982. Decades later, the series stil...   \n",
       "7  Each article is the product of extensive resea...   \n",
       "8  The book you now hold was originally published...   \n",
       "9  However, the chapters have been completely rev...   \n",
       "\n",
       "                                  Emoji_Removed_urdu  \n",
       "0                 میں اپنے والدین سے کیسے بات کروں ؟  \n",
       "1                             میں دوست کیسے بنائوں ؟  \n",
       "2                           میں اتنا اداس کیوں ہوں؟.  \n",
       "3  اگر آپ نے اپنے آپ سے ایسے سوالات کیے ہیں، تو آ...  \n",
       "4   اس بات پر منحصر ہے کہ آپ رہنمائی کے لیے کہاں ...  \n",
       "5  نوجوانوں کو ٹھوس مشورے حاصل کرنے میں مدد کرنے ...  \n",
       "6  8 جنوری 1982۔ دہائیوں کے بعد، سیریز اب بھی ایک...  \n",
       "7  درحقیقت، اس بات کا تعین کرنے کے لیے کہ نوجوان ...  \n",
       "8  جو کتاب آپ کے پاس ہے وہ اصل میں 1989 میں شائع ...  \n",
       "9                                                ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Update function to handle non-string values\n",
    "def remove_emoji_and_links(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Remove emojis using regex\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Remove links using regex\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "# Apply the function to both English and Urdu columns\n",
    "df['Emoji_Removed_english'] = df['SENTENCES '].apply(remove_emoji_and_links)\n",
    "df['Emoji_Removed_urdu'] = df['MEANING'].apply(remove_emoji_and_links)\n",
    "\n",
    "# Display the cleaned columns\n",
    "df[['Emoji_Removed_english', 'Emoji_Removed_urdu']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sami\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Sami\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'xx_ent_wiki_sm' (3.7.0) was trained with spaCy v3.7.0 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   tokenized_english  \\\n",
      "0   [How, can, I, communicate, with, my, parents, ?]   \n",
      "1                 [How, can, I, make, friends, ?, ’]   \n",
      "2                   [Why, do, I, get, so, sad, ?, ’]   \n",
      "3  [If, you, ’ve, asked, yourself, such, question...   \n",
      "4  [Depending, on, where, you, ’ve, turned, for, ...   \n",
      "5  [To, help, young, people, get, solid, advice, ...   \n",
      "6  [in, January1982, ., Decades, later, ,, the, s...   \n",
      "7  [Each, article, is, the, product, of, extensiv...   \n",
      "8  [The, book, you, now, hold, was, originally, p...   \n",
      "9  [However, ,, the, chapters, have, been, comple...   \n",
      "\n",
      "                                      tokenized_urdu  \n",
      "0        [میں, اپنے, والدین, سے, کیسے, بات, کروں, ؟]  \n",
      "1                       [میں, دوست, کیسے, بنائوں, ؟]  \n",
      "2                 [میں, اتنا, اداس, کیوں, ہوں, ؟, .]  \n",
      "3  [اگر, آپ, نے, اپنے, آپ, سے, ایسے, سوالات, کیے,...  \n",
      "4  [ , اس, بات, پر, منحصر, ہے, کہ, آپ, رہنمائی, ک...  \n",
      "5  [نوجوانوں, کو, ٹھوس, مشورے, حاصل, کرنے, میں, م...  \n",
      "6  [8, جنوری, 1982, ۔, دہائیوں, کے, بعد, ،, سیریز...  \n",
      "7  [درحقیقت, ،, اس, بات, کا, تعین, کرنے, کے, لیے,...  \n",
      "8  [جو, کتاب, آپ, کے, پاس, ہے, وہ, اصل, میں, 1989...  \n",
      "9  [                                             ...  \n",
      "English Vocab Size: 19322\n",
      "Urdu Vocab Size: 16401\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the multilingual model\n",
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "# Tokenization function using SpaCy\n",
    "def tokenize_urdu(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# English tokenizer using spaCy\n",
    "def tokenize_text_spacy(text):\n",
    "    return [token.text for token in nlp_en(text)]\n",
    "\n",
    "# Example DataFrame with English text\n",
    "# Assuming df is the DataFrame containing your data with a column 'Emoji_Removed_english'\n",
    "# Apply tokenization to English column\n",
    "df['tokenized_english'] = df['Emoji_Removed_english'].apply(tokenize_text_spacy)\n",
    "df['tokenized_urdu'] = df['Emoji_Removed_urdu'].apply(tokenize_urdu)\n",
    "\n",
    "# Function to create a word-to-index dictionary\n",
    "def create_vocab(tokenized_sentences):\n",
    "    word_to_index = defaultdict(lambda: len(word_to_index))\n",
    "    word_to_index['<PAD>'] = 0  # Adding padding token\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            _ = word_to_index[word]  # Assign index\n",
    "    return dict(word_to_index)\n",
    "\n",
    "# Create vocabulary for English\n",
    "english_vocab = create_vocab(df['tokenized_english'])\n",
    "urdu_vocab=create_vocab(df['tokenized_urdu'])\n",
    "\n",
    "# Display tokenized sentences and vocab size\n",
    "print(df[['tokenized_english','tokenized_urdu']].head(10))\n",
    "print(f\"English Vocab Size: {len(english_vocab)}\")\n",
    "print(f\"Urdu Vocab Size: {len(urdu_vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded_english: torch.Size([30164, 20])\n",
      "Shape of padded_urdu: torch.Size([30164, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ensure df has the required columns\n",
    "required_columns = ['tokenized_english', 'tokenized_urdu']\n",
    "if not all(col in df.columns for col in required_columns):\n",
    "    raise ValueError(\"DataFrame is missing required columns\")\n",
    "\n",
    "# Assuming english_vocab and urdu_vocab are already defined\n",
    "# If they're not, you'll need to create them first\n",
    "\n",
    "# Convert tokenized sentences to index sequences\n",
    "def convert_to_indices(vocab, sequences):\n",
    "    return [[vocab.get(word, 0) for word in sentence] for sentence in sequences]\n",
    "\n",
    "english_sequences = convert_to_indices(english_vocab, df['tokenized_english'])\n",
    "urdu_sequences = convert_to_indices(urdu_vocab, df['tokenized_urdu'])\n",
    "\n",
    "# Convert sequences to PyTorch tensors\n",
    "english_sequences = [torch.tensor(seq, dtype=torch.long) for seq in english_sequences]\n",
    "urdu_sequences = [torch.tensor(seq, dtype=torch.long) for seq in urdu_sequences]\n",
    "\n",
    "# Pad sequences using PyTorch's pad_sequence function\n",
    "def pad_sequences_pytorch(sequences, maxlen, padding='post'):\n",
    "    # First truncate sequences longer than maxlen\n",
    "    truncated_sequences = [seq[:maxlen] if len(seq) > maxlen else seq for seq in sequences]\n",
    "    \n",
    "    # Use pad_sequence to pad them to the same length\n",
    "    if padding == 'post':\n",
    "        padded_sequences = torch.nn.utils.rnn.pad_sequence(truncated_sequences, batch_first=True, padding_value=0)\n",
    "    else:  # If pre-padding is required\n",
    "        padded_sequences = torch.nn.utils.rnn.pad_sequence(truncated_sequences, batch_first=True, padding_value=0)\n",
    "        padded_sequences = torch.flip(padded_sequences, dims=[1])\n",
    "    \n",
    "    # Make sure the output shape is (num_sequences, maxlen)\n",
    "    return padded_sequences[:, :maxlen]\n",
    "\n",
    "# Set maximum sequence length\n",
    "max_length = 20  # Adjust this based on your dataset\n",
    "\n",
    "# Pad English and Urdu sequences\n",
    "padded_english = pad_sequences_pytorch(english_sequences, maxlen=max_length, padding='post')\n",
    "padded_urdu = pad_sequences_pytorch(urdu_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Handle zero-length sequences directly by masking any rows that are all zeros\n",
    "empty_mask = torch.all(padded_english == 0, dim=1)\n",
    "\n",
    "# Apply mask to both padded_english and padded_urdu to ensure consistency\n",
    "padded_english[empty_mask] = 0\n",
    "padded_urdu[empty_mask] = 0\n",
    "\n",
    "print(f\"Shape of padded_english: {padded_english.shape}\")\n",
    "print(f\"Shape of padded_urdu: {padded_urdu.shape}\")\n",
    "\n",
    "# Now padded_english and padded_urdu are ready for model training in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 24131\n",
      "Validation set size: 3016\n",
      "Test set size: 3017\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df contains your dataset with tokenized sentences\n",
    "\n",
    "# Shuffle the dataset using random permutation\n",
    "shuffled_indices = np.random.permutation(len(df))\n",
    "\n",
    "# Calculate split indices\n",
    "train_end = int(len(df) * 0.8)  # 80% for training\n",
    "val_end = train_end + int(len(df) * 0.1)  # 10% for validation\n",
    "\n",
    "# Create training, validation, and test sets using the shuffled indices\n",
    "train_indices = shuffled_indices[:train_end]\n",
    "val_indices = shuffled_indices[train_end:val_end]\n",
    "test_indices = shuffled_indices[val_end:]\n",
    "\n",
    "train_df = df.iloc[train_indices]\n",
    "val_df = df.iloc[val_indices]\n",
    "test_df = df.iloc[test_indices]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Optional: Reset indices if needed\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.6605\n",
      "Epoch [2/10], Loss: 3.1092\n",
      "Epoch [3/10], Loss: 2.9016\n",
      "Epoch [4/10], Loss: 2.7465\n",
      "Epoch [5/10], Loss: 2.6226\n",
      "Epoch [6/10], Loss: 2.5164\n",
      "Epoch [7/10], Loss: 2.4262\n",
      "Epoch [8/10], Loss: 2.3480\n",
      "Epoch [9/10], Loss: 2.2788\n",
      "Epoch [10/10], Loss: 2.2161\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english_sequences, urdu_sequences):\n",
    "        self.english_sequences = english_sequences\n",
    "        self.urdu_sequences = urdu_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sequences[idx], self.urdu_sequences[idx]\n",
    "\n",
    "# Define the Simple RNN Model\n",
    "class SimpleRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(SimpleRNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Input: [batch_size, seq_length]\n",
    "        x, _ = self.rnn(x)  # Output: [batch_size, seq_length, hidden_dim]\n",
    "        x = self.fc(x)  # Apply the linear layer\n",
    "        return x  # Return output for all time steps\n",
    "\n",
    "# Initialize hyperparameters\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = TranslationDataset(padded_english, padded_urdu)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleRNNModel(vocab_size=len(english_vocab), embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=len(urdu_vocab))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "\n",
    "        # Reshape outputs and targets for loss calculation\n",
    "        outputs = outputs.view(-1, len(urdu_vocab))  # Flatten outputs to [batch_size * max_length, output_size]\n",
    "        targets = targets.view(-1)  # Flatten targets to match outputs shape\n",
    "\n",
    "        loss = criterion(outputs, targets)  # Calculate loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)  # Average loss for the epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Model evaluation can be added here using validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0559\n",
      "Validation Accuracy: 61.92%\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a validation dataset (padded_urdu and padded_english for validation)\n",
    "# Create a DataLoader for the validation set using the validation DataFrame\n",
    "val_indices = val_df.index.tolist()  # Assuming val_df contains your validation indices\n",
    "val_english_sequences = [padded_english[i] for i in val_indices]\n",
    "val_urdu_sequences = [padded_urdu[i] for i in val_indices]\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_dataset = TranslationDataset(val_english_sequences, val_urdu_sequences)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "\n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            outputs = outputs.view(-1, len(urdu_vocab))  # Flatten outputs\n",
    "            targets = targets.view(-1)  # Flatten targets\n",
    "\n",
    "            loss = criterion(outputs, targets)  # Calculate loss\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "            # Calculate predictions and accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            total_correct += (predicted == targets).sum().item()  # Count correct predictions\n",
    "            total_samples += targets.size(0)  # Count total samples\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)  # Average loss\n",
    "    accuracy = total_correct / total_samples * 100  # Accuracy in percentage\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = evaluate_model(model, val_dataloader, criterion)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0562\n",
      "Test Accuracy: 61.93%\n",
      "English: How can I communicate with my parents ?\n",
      "Predicted Urdu: آپ\n",
      "\n",
      "English: How can I make friends ? ’\n",
      "Predicted Urdu: آپ\n",
      "\n",
      "English: Why do I get so sad ? ’\n",
      "Predicted Urdu: میں\n",
      "\n",
      "English: If you ’ve asked yourself such questions , you ’re not alone .\n",
      "Predicted Urdu: کے\n",
      "\n",
      "English: Depending on where you ’ve turned for guidance , you may have been given conflicting answers .\n",
      "Predicted Urdu: کے\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the test set using the test DataFrame\n",
    "test_indices = test_df.index.tolist()  # Assuming test_df contains your test indices\n",
    "test_english_sequences = [padded_english[i] for i in test_indices]\n",
    "test_urdu_sequences = [padded_urdu[i] for i in test_indices]\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_dataset = TranslationDataset(test_english_sequences, test_urdu_sequences)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "\n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            outputs = outputs.view(-1, len(urdu_vocab))  # Flatten outputs\n",
    "            targets = targets.view(-1)  # Flatten targets\n",
    "\n",
    "            loss = criterion(outputs, targets)  # Calculate loss\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "            # Calculate predictions and accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            total_correct += (predicted == targets).sum().item()  # Count correct predictions\n",
    "            total_samples += targets.size(0)  # Count total samples\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)  # Average loss\n",
    "    accuracy = total_correct / total_samples * 100  # Accuracy in percentage\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = evaluate_model(model, test_dataloader, criterion)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Example translations for a few test sequences\n",
    "def translate_examples(model, dataloader, english_vocab, urdu_vocab, num_examples=5):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, len(urdu_vocab))\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted indices\n",
    "            \n",
    "            # Get the original English sentences and predicted Urdu sentences\n",
    "            for j in range(len(inputs)):\n",
    "                if i * dataloader.batch_size + j >= num_examples:\n",
    "                    break  # Stop if we've collected enough examples\n",
    "                \n",
    "                # Convert indices back to words\n",
    "                english_sentence = ' '.join([list(english_vocab.keys())[list(english_vocab.values()).index(word.item())] for word in inputs[j] if word.item() != 0])\n",
    "                urdu_translation = ' '.join([list(urdu_vocab.keys())[list(urdu_vocab.values()).index(predicted[i * dataloader.batch_size + j].item())]])\n",
    "                \n",
    "                translations.append((english_sentence, urdu_translation))\n",
    "    \n",
    "    return translations\n",
    "\n",
    "# Get example translations\n",
    "example_translations = translate_examples(model, test_dataloader, english_vocab, urdu_vocab)\n",
    "\n",
    "# Print example translations\n",
    "for eng, urdu in example_translations:\n",
    "    print(f\"English: {eng}\\nPredicted Urdu: {urdu}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Input: [batch_size, seq_length]\n",
    "        x, _ = self.lstm(x)  # Output: [batch_size, seq_length, hidden_dim]\n",
    "        x = self.fc(x)  # Apply the linear layer\n",
    "        return x  # Return output for all time steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.7274\n",
      "Epoch [2/10], Loss: 3.1522\n",
      "Epoch [3/10], Loss: 2.9430\n",
      "Epoch [4/10], Loss: 2.7722\n",
      "Epoch [5/10], Loss: 2.6264\n",
      "Epoch [6/10], Loss: 2.4983\n",
      "Epoch [7/10], Loss: 2.3841\n",
      "Epoch [8/10], Loss: 2.2829\n",
      "Epoch [9/10], Loss: 2.1923\n",
      "Epoch [10/10], Loss: 2.1119\n"
     ]
    }
   ],
   "source": [
    "# Initialize hyperparameters\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = TranslationDataset(padded_english, padded_urdu)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the LSTM model, loss function, and optimizer\n",
    "lstm_model = LSTMModel(vocab_size=len(english_vocab), embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=len(urdu_vocab))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop for LSTM\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = lstm_model(inputs)  # Forward pass\n",
    "\n",
    "        # Reshape outputs and targets for loss calculation\n",
    "        outputs = outputs.view(-1, len(urdu_vocab))  # Flatten outputs\n",
    "        targets = targets.view(-1)  # Flatten targets\n",
    "\n",
    "        loss = criterion(outputs, targets)  # Calculate loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)  # Average loss for the epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0562\n",
      "Test Accuracy: 61.93%\n",
      "English: How can I communicate with my parents ?\n",
      "Predicted Urdu: آپ\n",
      "\n",
      "English: How can I make friends ? ’\n",
      "Predicted Urdu: آپ\n",
      "\n",
      "English: Why do I get so sad ? ’\n",
      "Predicted Urdu: میں\n",
      "\n",
      "English: If you ’ve asked yourself such questions , you ’re not alone .\n",
      "Predicted Urdu: کے\n",
      "\n",
      "English: Depending on where you ’ve turned for guidance , you may have been given conflicting answers .\n",
      "Predicted Urdu: کے\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the test set using the test DataFrame\n",
    "test_indices = test_df.index.tolist()  # Assuming test_df contains your test indices\n",
    "test_english_sequences = [padded_english[i] for i in test_indices]\n",
    "test_urdu_sequences = [padded_urdu[i] for i in test_indices]\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_dataset = TranslationDataset(test_english_sequences, test_urdu_sequences)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "\n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            outputs = outputs.view(-1, len(urdu_vocab))  # Flatten outputs\n",
    "            targets = targets.view(-1)  # Flatten targets\n",
    "\n",
    "            loss = criterion(outputs, targets)  # Calculate loss\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "            # Calculate predictions and accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            total_correct += (predicted == targets).sum().item()  # Count correct predictions\n",
    "            total_samples += targets.size(0)  # Count total samples\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)  # Average loss\n",
    "    accuracy = total_correct / total_samples * 100  # Accuracy in percentage\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = evaluate_model(model, test_dataloader, criterion)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Example translations for a few test sequences\n",
    "def translate_examples(model, dataloader, english_vocab, urdu_vocab, num_examples=5):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, len(urdu_vocab))\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted indices\n",
    "            \n",
    "            # Get the original English sentences and predicted Urdu sentences\n",
    "            for j in range(len(inputs)):\n",
    "                if i * dataloader.batch_size + j >= num_examples:\n",
    "                    break  # Stop if we've collected enough examples\n",
    "                \n",
    "                # Convert indices back to words\n",
    "                english_sentence = ' '.join([list(english_vocab.keys())[list(english_vocab.values()).index(word.item())] for word in inputs[j] if word.item() != 0])\n",
    "                urdu_translation = ' '.join([list(urdu_vocab.keys())[list(urdu_vocab.values()).index(predicted[i * dataloader.batch_size + j].item())]])\n",
    "                \n",
    "                translations.append((english_sentence, urdu_translation))\n",
    "    \n",
    "    return translations\n",
    "\n",
    "# Get example translations\n",
    "example_translations = translate_examples(model, test_dataloader, english_vocab, urdu_vocab)\n",
    "\n",
    "# Print example translations\n",
    "for eng, urdu in example_translations:\n",
    "    print(f\"English: {eng}\\nPredicted Urdu: {urdu}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
